{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206e9e6d-b294-4c21-af95-fa4e4820537b",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707bbe8-593a-409f-a0bf-38a4ea3b3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given probabilities\n",
    "p_H = 0.7  # Probability that an employee uses the health insurance plan\n",
    "p_S_not = 0.3  # Probability that an employee is not a smoker\n",
    "p_H_given_S = 0.4  # Probability that an employee uses the health insurance plan given that he/she is a smoker\n",
    "\n",
    "# Calculate P(S), the probability that an employee is a smoker\n",
    "p_S = 1 - p_S_not\n",
    "\n",
    "# Calculate P(S|H) using Bayes' theorem\n",
    "p_S_given_H = (p_H_given_S * p_S) / p_H\n",
    "\n",
    "# Output the result\n",
    "print(f\"The probability that an employee is a smoker given that he/she uses the health insurance plan is: {p_S_given_H:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b381977-02ea-4d87-b17b-9edb457d3ca4",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Ans:-\r\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the features they are designed to handle:\r\n",
    "\r\n",
    "Bernoulli Naive Bayes:\r\n",
    "\r\n",
    "Assumes that the features are binary, meaning they can take only two possible values (typically 0 or 1).\r\n",
    "Commonly used for text classification problems where the presence or absence of words in a document is considered.\r\n",
    "Well-suited for problems where the feature vectors represent binary data, such as document classification where each word is treated as a binary feature.\r\n",
    "Multinomial Naive Bayes:\r\n",
    "\r\n",
    "Assumes that the features represent counts or frequencies of events.\r\n",
    "Often used for text classification problems where the features are the counts of word occurrences in a document.\r\n",
    "Suitable for problems where the feature vectors represent discrete data (e.g., word counts) and are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82b03e-9a9d-4de2-94f9-15bae3c9cf84",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Ans:-Bernoulli Naive Bayes, like other Naive Bayes variants, does not handle missing values in a straightforward manner. It assumes that all features are binary, meaning they take values of 0 or 1. In the context of text classification, for example, a feature might represent the presence or absence of a particular word in a document.\r\n",
    "\r\n",
    "If you have missing values in your data and you are using Bernoulli Naive Bayes, you might need to handle them before applying the classifier. Here are a couple of common strategies:\r\n",
    "\r\n",
    "Imputation:\r\n",
    "\r\n",
    "You can impute the missing values by replacing them with a suitable value (e.g., 0 or 1).\r\n",
    "The choice of imputation depends on the nature of your data and the meaning of the missing values.\r\n",
    "Feature Engineering:\r\n",
    "\r\n",
    "If missing values are common and imputation is not straightforward, you might consider creating an additional binary feature to explicitly indicate whether a value is missing or not.\r\n",
    "This way, the missing information is not lost, and the model can potentially learn how to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b0d49-294a-4acc-ae64-5ef7cd2193c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Example data with missing values\n",
    "X_train = np.array([[1, 1, 0], [0, np.nan, 1], [1, 0, 1], [0, 1, 0]])\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Create a pipeline with a Bernoulli Naive Bayes classifier and a simple imputer\n",
    "model = make_pipeline(SimpleImputer(strategy='most_frequent'), BernoulliNB())\n",
    "\n",
    "# Fit the model on the data with missing values\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Example test data with missing values\n",
    "X_test = np.array([[1, np.nan, 0]])\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(f\"Predictions for the test data: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d1a3c-32a7-4de1-b325-3c9c5eb60745",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421e8af-9320-4ae1-ac27-3b9eaacb0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset (a well-known multi-class dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and display classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# Display detailed classification report\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
